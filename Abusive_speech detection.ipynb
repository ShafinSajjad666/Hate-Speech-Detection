{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXi45a6LUuoZOc+qqhFR1j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pandas sklearn nltk imblearn fasttext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPjDon83sm4G","executionInfo":{"status":"ok","timestamp":1727111344334,"user_tz":-360,"elapsed":1624,"user":{"displayName":"Sajjad Ullah","userId":"15155098567417613944"}},"outputId":"8f0a74fc-2e08-444d-a296-0ebde13465a6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Collecting sklearn\n","  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"cell_type":"code","source":["!pip install fasttext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czb3x64Ks9zy","executionInfo":{"status":"ok","timestamp":1727111430998,"user_tz":-360,"elapsed":84542,"user":{"displayName":"Sajjad Ullah","userId":"15155098567417613944"}},"outputId":"d376d910-b21f-4828-f51d-cedc36baaac9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fasttext\n","  Downloading fasttext-0.9.3.tar.gz (73 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11>=2.2 (from fasttext)\n","  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n","Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296184 sha256=27eed9a9817a1f37d26fadc940ff95890ed2a7160fe4cd73b706dd2d2d3d363c\n","  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24sgnUoLszF1","executionInfo":{"status":"ok","timestamp":1727111441572,"user_tz":-360,"elapsed":4281,"user":{"displayName":"Sajjad Ullah","userId":"15155098567417613944"}},"outputId":"d1b99ae1-6b85-4393-f214-718caaf5253e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0j97p7eXqYyH","executionInfo":{"status":"ok","timestamp":1727111480660,"user_tz":-360,"elapsed":37339,"user":{"displayName":"Sajjad Ullah","userId":"15155098567417613944"}},"outputId":"77904886-b27e-4513-ec8b-592b18f86da5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution in the dataset:\n","label\n","Personal          4225\n","Geopolitical      3761\n","Religious         1714\n","Political         1596\n","Neutral            835\n","Gender abusive     316\n","Name: count, dtype: int64\n","Class 'Personal': 4225 samples\n","Class 'Geopolitical': 3761 samples\n","Class 'Religious': 1714 samples\n","Class 'Political': 1596 samples\n","Class 'Neutral': 835 samples\n","Class 'Gender abusive': 316 samples\n","Class distribution before SMOTE:\n","label_encoded\n","3    4225\n","1    3761\n","5    1714\n","4    1596\n","2     835\n","0     316\n","Name: count, dtype: int64\n","\n","Class distribution after SMOTE:\n","{0: 4225, 1: 4225, 2: 4225, 3: 4225, 4: 4225, 5: 4225}\n","\n","Training data shape: (20280, 10000)\n","Test data shape: (5070, 10000)\n"]}],"source":["# Importing necessary libraries\n","import pandas as pd\n","import re\n","import string\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from imblearn.over_sampling import SMOTE\n","import numpy as np\n","\n","# Load dataset\n","df = pd.read_csv('/content/dataset.csv')  # Adjust path to your dataset\n","\n","# Assuming the class labels are in the 'label' column\n","class_distribution = df['label'].value_counts()\n","\n","# Display the class distribution\n","print(\"Class distribution in the dataset:\")\n","print(class_distribution)\n","\n","# If you want to display the class names along with their counts\n","for label, count in class_distribution.items():\n","    print(f\"Class '{label}': {count} samples\")\n","\n","# Bangla and Banglish stopwords\n","bangla_stopwords = set(stopwords.words('bengali'))\n","banglish_stopwords = set(stopwords.words('english'))\n","\n","# Cleaning text function\n","def clean_text(text, language='banglish'):\n","    # Lowercasing\n","    text = text.lower()\n","\n","    # Remove punctuation and special characters\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","\n","    # Remove stopwords\n","    if language == 'bangla':\n","        tokens = word_tokenize(text)\n","        text = ' '.join([word for word in tokens if word not in bangla_stopwords])\n","    elif language == 'banglish':\n","        tokens = word_tokenize(text)\n","        text = ' '.join([word for word in tokens if word not in banglish_stopwords])\n","\n","    return text\n","\n","# Applying cleaning function\n","df['clean_bangla'] = df['bangla'].apply(lambda x: clean_text(x, language='bangla'))\n","df['clean_banglish'] = df['banglish'].apply(lambda x: clean_text(x, language='banglish'))\n","\n","# Convert categorical labels to numerical labels\n","label_encoder = LabelEncoder()\n","df['label_encoded'] = label_encoder.fit_transform(df['label'])\n","\n","# Check class distribution before SMOTE\n","print(\"Class distribution before SMOTE:\")\n","print(df['label_encoded'].value_counts())\n","\n","# TF-IDF Vectorization (you can use for both bangla and banglish text separately)\n","tfidf_vectorizer_bangla = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n","tfidf_vectorizer_banglish = TfidfVectorizer(max_features=5000)\n","\n","# Vectorize the bangla and banglish columns\n","X_bangla_tfidf = tfidf_vectorizer_bangla.fit_transform(df['clean_bangla']).toarray()\n","X_banglish_tfidf = tfidf_vectorizer_banglish.fit_transform(df['clean_banglish']).toarray()\n","\n","# Combining the two vectors\n","X_combined = np.hstack((X_bangla_tfidf, X_banglish_tfidf))\n","\n","# Labels (y)\n","y = df['label_encoded']\n","\n","# Applying SMOTE for balancing the dataset\n","sm = SMOTE(random_state=42)\n","X_res, y_res = sm.fit_resample(X_combined, y)\n","\n","# Check class distribution after SMOTE\n","print(\"\\nClass distribution after SMOTE:\")\n","unique, counts = np.unique(y_res, return_counts=True)\n","print(dict(zip(unique, counts)))\n","\n","# Train Test Split (with balanced data)\n","X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n","\n","# Display shapes to confirm\n","print(\"\\nTraining data shape:\", X_train.shape)\n","print(\"Test data shape:\", X_test.shape)\n","\n","# Now you can use X_train, X_test, y_train, and y_test for training your model.\n"]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","\n","# Train Naive Bayes Classifier\n","nb_model = MultinomialNB()\n","nb_model.fit(X_train, y_train)\n","\n","# Predictions and accuracy\n","y_pred_nb = nb_model.predict(X_test)\n","accuracy_nb = accuracy_score(y_test, y_pred_nb)\n","print(f\"Naive Bayes Accuracy: {accuracy_nb * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJCONJh42Jpt","executionInfo":{"status":"ok","timestamp":1727111510240,"user_tz":-360,"elapsed":1439,"user":{"displayName":"Sajjad Ullah","userId":"15155098567417613944"}},"outputId":"5d5edfc9-dbe9-49c4-ea23-d9769cf02ee5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Bayes Accuracy: 84.95%\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from sklearn.metrics import accuracy_score\n","\n","# Tokenize the text data\n","tokenizer = Tokenizer(num_words=10000)  # Adjust num_words based on your dataset size\n","tokenizer.fit_on_texts(df['clean_bangla'].values)  # Tokenize on the clean Bangla text (or combined text if needed)\n","\n","# Convert text to sequences\n","X_bangla_seq = tokenizer.texts_to_sequences(df['clean_bangla'].values)\n","X_banglish_seq = tokenizer.texts_to_sequences(df['clean_banglish'].values)\n","\n","# Padding the sequences to ensure uniform input length\n","X_bangla_seq_padded = pad_sequences(X_bangla_seq, maxlen=100)  # Adjust maxlen based on text length\n","X_banglish_seq_padded = pad_sequences(X_banglish_seq, maxlen=100)\n","\n","# Combining Bangla and Banglish padded sequences\n","X_combined_seq = np.hstack((X_bangla_seq_padded, X_banglish_seq_padded))\n","\n","# Splitting the data into train and test sets\n","X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_combined_seq, df['label_encoded'], test_size=0.2, random_state=42)\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(Embedding(input_dim=10000, output_dim=128, input_length=X_train_seq.shape[1]))  # Embedding layer\n","model.add(LSTM(128, return_sequences=False))  # LSTM layer\n","model.add(Dropout(0.2))\n","model.add(Dense(64, activation='relu'))  # Dense layer\n","model.add(Dropout(0.2))\n","model.add(Dense(4, activation='softmax'))  # Output layer with 4 classes\n","\n","# Compile the model\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the LSTM model\n","model.fit(X_train_seq, y_train_seq, epochs=5, batch_size=32, validation_data=(X_test_seq, y_test_seq))\n","\n","# Evaluate the model\n","score, accuracy = model.evaluate(X_test_seq, y_test_seq)\n","print(f\"LSTM Test Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Yr_AciqR32ui","executionInfo":{"status":"error","timestamp":1727114783305,"user_tz":-360,"elapsed":5388,"user":{"displayName":"Sajjad Ullah","userId":"15155098567417613944"}},"outputId":"8fcf5328-740b-48e4-839b-fdd9b4d86271"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-10-1128f10cc84c>\", line 39, in <cell line: 39>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 357, in _compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 325, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 609, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 645, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\", line 43, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 1853, in sparse_categorical_crossentropy\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py\", line 1567, in sparse_categorical_crossentropy\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py\", line 645, in sparse_categorical_crossentropy\n\nReceived a label value of 5 which is outside the valid range of [0, 4).  Label values: 1 4 1 3 5 4 2 2 1 1 1 3 4 3 1 1 4 5 3 2 1 3 3 1 1 1 3 4 1 4 3 3\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_6198]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1128f10cc84c>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Train the LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-10-1128f10cc84c>\", line 39, in <cell line: 39>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 357, in _compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 325, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 609, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 645, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\", line 43, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 1853, in sparse_categorical_crossentropy\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py\", line 1567, in sparse_categorical_crossentropy\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py\", line 645, in sparse_categorical_crossentropy\n\nReceived a label value of 5 which is outside the valid range of [0, 4).  Label values: 1 4 1 3 5 4 2 2 1 1 1 3 4 3 1 1 4 5 3 2 1 3 3 1 1 1 3 4 1 4 3 3\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_6198]"]}]}]}